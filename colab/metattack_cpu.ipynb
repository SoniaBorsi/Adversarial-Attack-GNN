{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1vaMENvwSVf+XTHHnq/Vm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries"
      ],
      "metadata": {
        "id": "pm7xp-cSpNU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ieFvWeGqpC-v"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall -y torch torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv deeprobust\n",
        "'''\n",
        "If import block fails with a numpy size diff, uncomment and run the two lines below. rerun all blocks after.\n",
        "'''\n",
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1\n",
        "!pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "!pip install deeprobust\n",
        "!pip install networkx matplotlib\n",
        "!pip install ogb"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TCMU_dwppLBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required modules\n",
        "import torch\n",
        "import torch_geometric\n",
        "import deeprobust\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"PyG Installed:\", torch_geometric.__version__)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "print(\"Numpy Installed:\", np.__version__)\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.sparse as sparse"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7O0SbtqJpPB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix, subgraph\n",
        "from torch_geometric.data import Data\n",
        "from torch_sparse import from_scipy\n",
        "\n",
        "from deeprobust.graph.global_attack import Metattack\n",
        "print(\"Metattack Imported Successfully\")\n",
        "\n",
        "# Planetoid: Cora, Citeseer, PubMed\n",
        "# WebKB: Texas\n",
        "# PolBlogs: polblogs\n",
        "# Flickr: Flickr\n",
        "from torch_geometric.datasets import Planetoid, WebKB, PolBlogs, Flickr\n",
        "# OGB: ogbn-proteins\n",
        "from ogb.nodeproppred import PygNodePropPredDataset"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SIWlwEs_phMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Datasets\n",
        "Cora, Citeseer, PubMed, Texas, PolBlogs, and Flickr"
      ],
      "metadata": {
        "id": "GtHaD2cxpmgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from torch_geometric.utils import subgraph\n",
        "from torch_geometric.data import Data\n",
        "import os\n",
        "\n",
        "def subset_graph(data, num_nodes_subset, save_path):\n",
        "  #Make a subset of a graph if it is too big\n",
        "  perm = torch.randperm(data.num_nodes)[:num_nodes_subset]\n",
        "  perm = perm.sort()[0]\n",
        "\n",
        "  edge_index, _ = subgraph(perm, data.edge_index, relabel_nodes=True)\n",
        "\n",
        "  x = data.x[perm] if data.x is not None else None\n",
        "  y = data.y[perm] if data.y is not None else None\n",
        "\n",
        "  new_data = Data(x=x, edge_index=edge_index, y=y)\n",
        "  torch.save(new_data, save_path)\n",
        "  print(f\"Saved subset graph to {save_path}\")\n",
        "  return new_data\n",
        "'''"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U1wVZrUhY8NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Still failing because using all RAM\n",
        "'''\n",
        "import random\n",
        "\n",
        "def subsample(data, num_nodes):\n",
        "  node_indices = torch.randperm(data.num_nodes)[:num_nodes]\n",
        "  edge_index, _ = subgraph(node_indices, data.edge_index, relabel_nodes=True)\n",
        "  data.x = data.x[node_indices]\n",
        "  data.y = data.y[node_indices]\n",
        "\n",
        "  # (Re)define masks\n",
        "  data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "  data.train_mask[:int(0.6 * num_nodes)] = True\n",
        "\n",
        "  data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "  data.val_mask[int(0.6 * num_nodes):int(0.8 * num_nodes)] = True\n",
        "\n",
        "  data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "  data.test_mask[int(0.8 * num_nodes):] = True\n",
        "\n",
        "  data.edge_index = edge_index\n",
        "  return data\n",
        "'''"
      ],
      "metadata": {
        "id": "Xw0KcQbw0twl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Still failing because uses all RAM\n",
        "\n",
        "'''\n",
        "def subsample_ogbn_proteins(dataset, num_nodes=7000):\n",
        "  full_data = dataset[0]\n",
        "  sampled_nodes = torch.randperm(full_data.num_nodes)[:num_nodes]\n",
        "\n",
        "  edge_index, edge_mask = subgraph(sampled_nodes, full_data.edge_index, relabel_nodes=True)\n",
        "  edge_attr = full_data.edge_attr[edge_mask]\n",
        "\n",
        "  # Simulate node features if missing\n",
        "  x = torch.zeros((num_nodes, 1))\n",
        "\n",
        "  # Subsample y\n",
        "  y = full_data.y[sampled_nodes]\n",
        "\n",
        "  # Generate new train/val/test masks\n",
        "  train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "  val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "  test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "  train_mask[:int(0.6 * num_nodes)] = True\n",
        "  val_mask[int(0.6 * num_nodes):int(0.8 * num_nodes)] = True\n",
        "  test_mask[int(0.8 * num_nodes):] = True\n",
        "\n",
        "  return Data(\n",
        "      x=x,\n",
        "      edge_index=edge_index,\n",
        "      edge_attr=edge_attr,\n",
        "      y=y,\n",
        "      train_mask=train_mask,\n",
        "      val_mask=val_mask,\n",
        "      test_mask=test_mask,\n",
        "  )\n",
        "'''"
      ],
      "metadata": {
        "id": "9WXN2EiL2Jwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_dataset(name):\n",
        "  if name in [\"Cora\", \"Citeseer\", \"PubMed\"]:\n",
        "    return Planetoid(root=f\"./data/{name}\", name=name)[0]\n",
        "  elif name == \"Texas\":\n",
        "    return WebKB(root=f\"./data/{name}\", name=name)[0]\n",
        "  elif name == \"PolBlogs\":\n",
        "    return PolBlogs(root=f\"./tmp/polblogs\")[0]\n",
        "  elif name == \"ogbn-proteins\":\n",
        "    return PygNodePropPredDataset(root=\"./data/ogb\", name=\"ogbn-proteins\")[0]\n",
        "  elif name == \"Flickr\":\n",
        "    return Flickr(root=f\"./data/Flickr\")[0]\n",
        "  else:\n",
        "    raise ValueError(\"Dataset not found\")\n",
        "\n",
        "datasets = {\n",
        "    \"Cora\": load_dataset(\"Cora\"),\n",
        "    \"Citeseer\": load_dataset(\"Citeseer\"),\n",
        "    \"PubMed\": load_dataset(\"PubMed\"),\n",
        "    \"PolBlogs\": load_dataset(\"PolBlogs\"),\n",
        "    \"Texas\": load_dataset(\"Texas\"),\n",
        "    \"ogbn-proteins\": load_dataset(\"ogbn-proteins\"),\n",
        "    \"Flickr\": load_dataset(\"Flickr\")\n",
        "}\n",
        "\n",
        "# Load datasets for all datasets when using ogbn-proteins, flickr, and pubmed\n",
        "'''\n",
        "def load_dataset(name):\n",
        "  if name in [\"Cora\", \"Citeseer\", \"PubMed\"]:\n",
        "    data = Planetoid(root=f\"./data/{name}\", name=name)[0]\n",
        "\n",
        "    if name == \"PubMed\":\n",
        "      data = subsample(data, 5000)\n",
        "    return data\n",
        "\n",
        "  elif name == \"Texas\":\n",
        "    return WebKB(root=f\"./data/{name}\", name=name)[0]\n",
        "  elif name == \"PolBlogs\":\n",
        "   return PolBlogs(root=f\"./tmp/polblogs\")[0]\n",
        "  elif name == \"ogbn-proteins\":\n",
        "    dataset = PygNodePropPredDataset(root=\"./data/ogb\", name=\"ogbn-proteins\")\n",
        "    return subsample_ogbn_proteins(dataset, num_nodes=7000)\n",
        "\n",
        "  elif name == \"Flickr\":\n",
        "    data = Flickr(root=f\"./data/Flickr\")[0]\n",
        "    return subsample(data, 5000)\n",
        "  else:\n",
        "    raise ValueError(\"Dataset not found\")\n",
        "\n",
        "datasets = {\n",
        "    \"Cora\": load_dataset(\"Cora\"),\n",
        "    \"Citeseer\": load_dataset(\"Citeseer\"),\n",
        "    \"PubMed\": load_dataset(\"PubMed\"),\n",
        "    \"PolBlogs\": load_dataset(\"PolBlogs\"),\n",
        "    \"Texas\": load_dataset(\"Texas\"),\n",
        "    \"ogbn-proteins\": load_dataset(\"ogbn-proteins\"),\n",
        "    \"Flickr\": load_dataset(\"Flickr\")\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HIKoXe9YphJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define GCN Model"
      ],
      "metadata": {
        "id": "-5C1-X5vp02b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_features, hidden_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.gc1 = GCNConv(num_features, hidden_channels)\n",
        "        self.gc2 = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "        self.nfeat = num_features\n",
        "        self.nclass = num_classes\n",
        "        self.hidden_sizes = [hidden_channels]\n",
        "        self.with_relu = True\n",
        "\n",
        "    @property\n",
        "    def gc1_weight(self):\n",
        "        return self.gc1.lin.weight\n",
        "\n",
        "    @property\n",
        "    def gc2_weight(self):\n",
        "        return self.gc2.lin.weight\n",
        "\n",
        "    @property\n",
        "    def weight(self):\n",
        "        return self.gc1.lin.weight\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.gc1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.gc2(x, edge_index)\n",
        "        self.output = x\n",
        "        return x"
      ],
      "metadata": {
        "id": "oX-qq5Hop3Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train GCN Model"
      ],
      "metadata": {
        "id": "tBWheMutp5rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data, epochs=100, lr=0.01, weight_decay=5e-4):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "iyj8k5OIp8D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metattack Over All Datasets"
      ],
      "metadata": {
        "id": "bm5azSBxp9X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_metattack(model, data, num_perturbations):\n",
        "  # Convert adjacency matrix and features to sparse format\n",
        "  adj = to_scipy_sparse_matrix(data.edge_index, num_nodes=data.num_nodes).tocsr()\n",
        "  features = sparse.csr_matrix(data.x.numpy())\n",
        "  labels = data.y.numpy()\n",
        "\n",
        "  # Get train and unlabeled indices\n",
        "  idx_train = data.train_mask.nonzero(as_tuple=True)[0].cpu().numpy()\n",
        "  idx_unlabeled = (~data.train_mask).nonzero(as_tuple=True)[0].cpu().numpy()\n",
        "\n",
        "  # Initialize Metattack\n",
        "  attacker = Metattack(\n",
        "      model=model,\n",
        "      nnodes=data.num_nodes,\n",
        "      attack_structure=True,\n",
        "      attack_features=False,\n",
        "      device='cpu'\n",
        "  )\n",
        "\n",
        "  # Perform attack\n",
        "  attacker.attack(\n",
        "      features,\n",
        "      adj,\n",
        "      labels,\n",
        "      idx_train,\n",
        "      idx_unlabeled,\n",
        "      n_perturbations=num_perturbations,\n",
        "      ll_constraint=False  # This avoids the sparse tensor issue\n",
        "  )\n",
        "\n",
        "  # Ensure `modified_adj` and `modified_features` exist and return them\n",
        "  modified_adj = attacker.modified_adj\n",
        "  modified_features = attacker.modified_features\n",
        "\n",
        "  # If Metattack doesn't return features, use the original\n",
        "  if modified_features is None:\n",
        "      modified_features = data.x\n",
        "\n",
        "  return attacker.modified_adj, attacker.modified_features"
      ],
      "metadata": {
        "id": "XUIUJ0dEqB7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model"
      ],
      "metadata": {
        "id": "XuQV1c66qGA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data, perturbed_adj, perturbed_features):\n",
        "  # Check the type of perturbed_adj\n",
        "  print(f\"perturbed_adj type: {type(perturbed_adj)}\")\n",
        "\n",
        "  # Ensure the adjacency matrix is in sparse COO format\n",
        "  if not perturbed_adj.is_sparse:\n",
        "        perturbed_adj = perturbed_adj.to_sparse()\n",
        "\n",
        "  # Get edge_index directly from the sparse tensor (COO format)\n",
        "  edge_index = perturbed_adj._indices()\n",
        "\n",
        "  # Use original features if perturbed ones are not returned\n",
        "  if perturbed_features is None:\n",
        "        perturbed_x = data.x\n",
        "  else:\n",
        "      if hasattr(perturbed_features, \"toarray\"):\n",
        "          perturbed_x = torch.tensor(perturbed_features.toarray(), dtype=torch.float)\n",
        "      else:\n",
        "          perturbed_x = perturbed_features\n",
        "\n",
        "  # Forward pass through the model using the perturbed graph and features\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "        out = model(perturbed_x, edge_index)\n",
        "\n",
        "  # Ensure test_mask is correctly sized and boolean\n",
        "  if data.test_mask.shape[0] != data.num_nodes:\n",
        "    print(f\"Adjusting test_mask size for {data.dataset_name}...\")\n",
        "    # Truncate or adjust the size\n",
        "    data.test_mask = data.test_mask[:data.num_nodes]\n",
        "\n",
        "  # Flatten test_mask if it's not 1D\n",
        "  if len(data.test_mask.shape) > 1:\n",
        "      data.test_mask = data.test_mask.flatten()\n",
        "\n",
        "  # Ensure data.test_mask has the correct size and is boolean\n",
        "  data.test_mask = data.test_mask[:data.num_nodes].to(torch.bool)\n",
        "\n",
        "  #if len(data.test_mask.shape) == 1:\n",
        "    # Ensure it is boolean\n",
        "    #data.test_mask = data.test_mask.to(torch.bool)\n",
        "\n",
        "  # Check the shapes of predictions and ground truth labels\n",
        "  print(f\"Prediction shape: {out.shape}\")\n",
        "  print(f\"test_mask shape: {data.test_mask.shape}\")\n",
        "  print(f\"Ground truth labels shape: {data.y.shape}\")\n",
        "\n",
        "  # Ensure data.y has the same size as test_mask\n",
        "  assert data.y.shape[0] == data.num_nodes, \"data.y shape mismatch with num_nodes\"\n",
        "\n",
        "  # Get predicted class labels\n",
        "  pred = out.argmax(dim=1)\n",
        "  # Compare predictions to ground truth\n",
        "  correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
        "  # Calculate accuracy\n",
        "  acc = correct / data.test_mask.sum().item()\n",
        "\n",
        "  return acc"
      ],
      "metadata": {
        "id": "T4lAhw2eqHbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "xqn-4UTMqIzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_graph(edge_index, title=\"Graph\"):\n",
        "  if isinstance(edge_index, torch.Tensor):\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "  if edge_index.shape[0] != 2:\n",
        "    raise ValueError(\"Expected edge_index shape [2, num_edges]\")\n",
        "\n",
        "  # Convert to list of edge tuples\n",
        "  edge_list = list(zip(edge_index[0], edge_index[1]))\n",
        "\n",
        "  G = nx.Graph()\n",
        "  G.add_edges_from(edge_list)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  nx.draw(G, node_size=30, edge_color=\"gray\", alpha=0.6, with_labels=False)\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "aWWuNPdhqKem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Masks"
      ],
      "metadata": {
        "id": "fiy3s9mkyfXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_masks(data, train_ratio=0.6, val_ratio=0.2):\n",
        "  num_nodes = data.num_nodes\n",
        "  indices = torch.randperm(num_nodes)\n",
        "\n",
        "  train_size = int(train_ratio * num_nodes)\n",
        "  val_size = int(val_ratio * num_nodes)\n",
        "\n",
        "  data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "  data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "  data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "  data.train_mask[indices[:train_size]] = True\n",
        "  data.val_mask[indices[train_size:train_size + val_size]] = True\n",
        "  data.test_mask[indices[train_size + val_size:]] = True\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "Wejt8Cw1yfGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Experiments"
      ],
      "metadata": {
        "id": "VD_uzhsbqMmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: make num_perturbations 0.2 * number of edges (once initial experiments run)\n",
        "\n",
        "def run_experiment(dataset_name):\n",
        "  dataset = load_dataset(dataset_name)\n",
        "\n",
        "  if isinstance(dataset, Data):\n",
        "    # Already a Data object\n",
        "    data = dataset\n",
        "  else:\n",
        "    # Dataset is a list-like object (like Planetoid)\n",
        "    data = dataset[0]\n",
        "\n",
        "  if data.x is None:\n",
        "    # Patching missing features (for PolBlogs, speficially)\n",
        "    # All zero-features carry no useful info -> poor performance,\n",
        "    # so each node has a unique identifier (one-hot encoding). GNN can differentiate them based on graph structure + node identity\n",
        "    print(f\"{dataset_name} patching missing features...\")\n",
        "    data.x = torch.eye(data.num_nodes)\n",
        "\n",
        "  # Check the shape of the masks and labels BEFORE: debug\n",
        "  #print(f\"train_mask shape: {data.train_mask.shape}\")\n",
        "  #print(f\"data.y shape: {data.y.shape}\")\n",
        "\n",
        "  if not hasattr(data, 'train_mask') or data.train_mask.shape[0] != data.num_nodes:\n",
        "    # dataset that doesnt come with masks ie. built-in train/test/val splits\n",
        "    print(f\"{dataset_name} making masks...\")\n",
        "    data = split_masks(data)\n",
        "  else:\n",
        "    if len(data.train_mask.shape) > 1:\n",
        "      # If the train_mask exists but is not 1D, fix it to be\n",
        "      print(f\"Fixing train_mask_shape for {dataset_name}...\")\n",
        "      data.train_mask = data.train_mask.view(-1)\n",
        "\n",
        "  # check train_mask is correct (Texas)\n",
        "  if data.train_mask.shape[0] != data.num_nodes:\n",
        "    print(f\"Adjusting train_masks size for {dataset_name}...\")\n",
        "    # Truncate or adjust the size\n",
        "    data.train_mask = data.train_mask[:data.num_nodes]\n",
        "  if len(data.train_mask.shape) == 1:\n",
        "    # Ensure it is boolean\n",
        "    data.train_mask = data.train_mask.to(torch.bool)\n",
        "\n",
        "  # Check the shape of the masks and labels AFTER: debug\n",
        "  #print(f\"train_mask shape: {data.train_mask.shape}\")\n",
        "  #print(f\"data.y shape: {data.y.shape}\")\n",
        "\n",
        "  num_classes = len(torch.unique(data.y))\n",
        "\n",
        "  print(\"=\" * 100)\n",
        "  print(f\"Dataset: {dataset_name}\\n\")\n",
        "  print(f\"Number of nodes: {data.num_nodes}\")\n",
        "  print(f\"Number of features: {data.num_features}\")\n",
        "  print(f\"Number of classes: {num_classes}\\n\")\n",
        "\n",
        "  model = GCN(data.num_features, 16, num_classes)\n",
        "  model = train_model(model, data)\n",
        "\n",
        "  if data.num_nodes <= 5000:\n",
        "    print(\"-\" * 100)\n",
        "    print(\"Original Graph\")\n",
        "    visualize_graph(data.edge_index, title=f\"{dataset_name}: Before Metattack\")\n",
        "\n",
        "  num_perturbations = int(data.edge_index.size(1) * 0.01)\n",
        "  print(f\"Running on {num_perturbations} perturbations\")\n",
        "  # TODO: make num_perturbations 0.2 * number of edges once initial experiments run\n",
        "  perturbed_adj, perturbed_features = apply_metattack(model, data, num_perturbations=num_perturbations)\n",
        "  edge_index_perturbed = perturbed_adj.nonzero().t()\n",
        "\n",
        "  if data.num_nodes <= 5000:\n",
        "    print(\"-\" * 100)\n",
        "    print(\"Perturbed Graph\")\n",
        "    visualize_graph(edge_index_perturbed, title=f\"{dataset_name}: After Metattack\")\n",
        "\n",
        "  acc = evaluate_model(model, data, perturbed_adj, perturbed_features)\n",
        "  print(f\"Accuracy after Metattack on {dataset_name}: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "gvFgB0LVqN4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution"
      ],
      "metadata": {
        "id": "8r9IL9t2qQhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all datasets, uncomment when debugged all\n",
        "#datasets = [\"Cora\", \"Citeseer\", \"PolBlogs\", \"Texas\", \"Flickr\", \"PubMed\", \"ogbn-proteins\"]\n",
        "\n",
        "# Base code these datasets run\n",
        "datasets = [\"Cora\", \"Citeseer\", \"PolBlogs\", \"Texas\"]\n",
        "# These datasets crash - reason: too big, use all available RAM\n",
        "#datasets = [ \"ogbn-proteins\", \"Flickr\", \"PubMed\"]\n",
        "\n",
        "for dataset_name in datasets:\n",
        "  run_experiment(dataset_name)"
      ],
      "metadata": {
        "id": "ND7BORaMqRpy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}